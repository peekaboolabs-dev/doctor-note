# ðŸš€ Trillion-7B ë¹ ë¥¸ ì‹œìž‘ ê°€ì´ë“œ

## ðŸ“‹ ì „ì²´ ì„¤ì • ìˆœì„œ

### 1ï¸âƒ£ llama.cpp ì„¤ì • (ìµœì´ˆ 1íšŒë§Œ)
```bash
# llama.cpp ì„¤ì¹˜ ë° ë¹Œë“œ
./scripts/setup_llamacpp.sh
```

ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆ˜í–‰í•˜ëŠ” ìž‘ì—…:
- llama.cpp í´ë¡  ë° ë¹Œë“œ (Mac Metal ìžë™ ê°ì§€)
- Python ë°”ì¸ë”© ì„¤ì¹˜ (llama-cpp-python)
- ì„œë²„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (run_llama_server.sh)

### 2ï¸âƒ£ Trillion-7B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
```bash
# models ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p models

# ì–‘ìží™” ë²„ì „ ë‹¤ìš´ë¡œë“œ (ê¶Œìž¥, ~4GB)
hf download trillionlabs/Trillion-7B-preview-GGUF trillion-7b-preview.q4_k_m.gguf --local-dir models/

# ë˜ëŠ” ë¹„ì–‘ìží™” ë²„ì „ (128GB M4 Max, ~14GB)
hf download trillionlabs/Trillion-7B-preview-GGUF trillion-7b-preview.bf16.gguf --local-dir models/
```

### 3ï¸âƒ£ llama.cpp ì„œë²„ ì‹¤í–‰
```bash
# ì–‘ìží™” ëª¨ë¸ë¡œ ì„œë²„ ì‹¤í–‰
./scripts/run_llama_server_clean.sh models/trillion-7b-preview.q4_k_m.gguf 8080

# ë˜ëŠ” ë¹„ì–‘ìží™” ëª¨ë¸
./scripts/run_llama_server_clean.sh models/trillion-7b-preview.bf16.gguf 8080
```

ì„œë²„ê°€ ì‹¤í–‰ë˜ë©´:
- http://localhost:8080 ì—ì„œ ì ‘ì† ê°€ëŠ¥
- API ì—”ë“œí¬ì¸íŠ¸: http://localhost:8080/completion

### 4ï¸âƒ£ Python í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python tests/test_llamacpp_trillion.py
```

## ðŸ” ìƒíƒœ í™•ì¸

### ì„œë²„ ìƒíƒœ í™•ì¸
```bash
# ì„œë²„ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
curl http://localhost:8080/health

# ëª¨ë¸ ì •ë³´ í™•ì¸
curl http://localhost:8080/props
```

### í¬íŠ¸ í™•ì¸
```bash
# 8080 í¬íŠ¸ ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸
lsof -i :8080
```

## ðŸ’¡ íŒ

### ë©”ëª¨ë¦¬ ê´€ë¦¬
- **Q4_K_M**: ~5GB RAM í•„ìš” (ê¶Œìž¥)
- **Q8_0**: ~8GB RAM í•„ìš”
- **BF16**: ~15GB RAM í•„ìš”

### ì„±ëŠ¥ ìµœì í™”
```bash
# GPU ë ˆì´ì–´ ì¡°ì • (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)
./llama.cpp/server -m models/trillion-7b-preview.q4_k_m.gguf -ngl 20

# ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ì¡°ì •
./llama.cpp/server -m models/trillion-7b-preview.q4_k_m.gguf -c 2048
```

## âš ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### "ì„œë²„ ì—°ê²° ì‹¤íŒ¨" ì˜¤ë¥˜
1. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
2. í¬íŠ¸ê°€ ì—´ë ¤ìžˆëŠ”ì§€ í™•ì¸
3. ë°©í™”ë²½ ì„¤ì • í™•ì¸

### "ë©”ëª¨ë¦¬ ë¶€ì¡±" ì˜¤ë¥˜
1. ë” ìž‘ì€ ì–‘ìží™” ëª¨ë¸ ì‚¬ìš© (q2_k, q3_k_m)
2. ë‹¤ë¥¸ í”„ë¡œê·¸ëž¨ ì¢…ë£Œ
3. GPU ë ˆì´ì–´ ìˆ˜ ê°ì†Œ (-ngl 10)

### "ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ" ì˜¤ë¥˜
1. íŒŒì¼ ê²½ë¡œ í™•ì¸
2. ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ì—¬ë¶€ í™•ì¸
3. íŒŒì¼ëª… ëŒ€ì†Œë¬¸ìž í™•ì¸

## ðŸ“ í™˜ê²½ë³€ìˆ˜ ì„¤ì • (.env)
```bash
# .env íŒŒì¼ ìƒì„±
cat > .env << EOF
LLM_TYPE=llamacpp_server
LLM_MODEL=trillion-7b
LLAMA_SERVER_HOST=localhost
LLAMA_SERVER_PORT=8080
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2048
EOF
```

## ðŸŽ¯ ì „ì²´ ëª…ë ¹ì–´ ìš”ì•½
```bash
# 1. ì´ˆê¸° ì„¤ì • (ìµœì´ˆ 1íšŒ)
./scripts/setup_llamacpp.sh

# 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
mkdir -p models
hf download trillionlabs/Trillion-7B-preview-GGUF trillion-7b-preview.q4_k_m.gguf --local-dir models/

# 3. ì„œë²„ ì‹¤í–‰
./scripts/run_llama_server_clean.sh models/trillion-7b-preview.q4_k_m.gguf 8080

# 4. ìƒˆ í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸
source venv/bin/activate
python tests/test_llamacpp_trillion.py
```

ì¤€ë¹„ ì™„ë£Œ! ðŸš€